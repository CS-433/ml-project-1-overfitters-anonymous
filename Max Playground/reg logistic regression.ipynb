{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf5e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "\n",
    "    >>> sigmoid(np.array([0.1]))\n",
    "    array([0.52497919])\n",
    "    >>> sigmoid(np.array([0.1, 0.1]))\n",
    "    array([0.52497919, 0.52497919])\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.e**t/(1+np.e**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4aab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> gradient\n",
    "    array([[-0.08370763],\n",
    "           [ 0.2467104 ],\n",
    "           [ 0.57712843]])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate log-loss L\n",
    "    \n",
    "    # New vectorized loss computation\n",
    "    sig_pred = sigmoid(tx @ w)\n",
    "    loss = -1 / len(y) * (y.T @ np.log(sig_pred) + (1 - y).T @ np.log(1 - sig_pred)).item() + (lambda_ / 2) * (w[1:].T @ w[1:]).item()\n",
    "    \n",
    "    \n",
    "    # Gradient with regularization (excluding bias term)\n",
    "    reg_term = np.copy(w)\n",
    "    reg_term[0] = 0  # No regularization for the bias term\n",
    "    gradient = 1 / len(y) * tx.T @ (sigmoid(tx @ w) - y) + lambda_ * reg_term\n",
    "    \n",
    "    #Calculating the hessian\n",
    "    #Creating S matrix with sigma values in diagonale\n",
    "    sigmas = []\n",
    "    for xi in tx:\n",
    "        sig_xi = sigmoid(xi.T @ w)\n",
    "        temp = sig_xi * (1 - sig_xi)\n",
    "        sigmas.append(temp.item())\n",
    "    \n",
    "    # Matrix with sig (1- sig) in diagonale\n",
    "    S = np.diag(sigmas)\n",
    "    hess_regul = np.identity(len(tx[0]))\n",
    "    hess_regul[0, 0] = 0 # Not regularizing the w0\n",
    "    hessian =  1 / len(y) * tx.T @ S @ tx + hess_regul\n",
    "    \n",
    "    \n",
    "    return loss, gradient, hessian\n",
    "\n",
    "def log_learning_by_penalized_gradient(y, tx, w, gamma, lambda_, hessian_w = False):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: scalar\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> w\n",
    "    array([[0.10837076],\n",
    "           [0.17532896],\n",
    "           [0.24228716]])\n",
    "    \"\"\"\n",
    "    \n",
    "    loss, gradient, hessian = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    if hessian_w == True:\n",
    "        w -= gamma * np.linalg.inv(hessian) @ gradient\n",
    "    else:\n",
    "        w -= gamma * gradient\n",
    "    \n",
    "    return w, loss\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Do logistic regression with regulatization until the threshold or max_iter is reached.\n",
    "    \n",
    "    Args:\n",
    "        y:            shape=(N, 1)\n",
    "        tx:           shape=(N, D+1)\n",
    "        initial_w:    shape=(D+1, 1)\n",
    "        max_iter:     int\n",
    "        gamma:        float\n",
    "        \n",
    "    Returns: \n",
    "        w:         shape=(D+1, 1)\n",
    "        loss:      loss at final w\n",
    "        \n",
    "    Example:\n",
    "    w_final, losses = reg_logistic_regression(y, tx, initial_w, max_iter, gamma)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build w\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        w, loss = learning_by_penalized_gradient(y.reshape(-1, 1), tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf4d5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Loss: 0.27624428668768497\n",
      "Logistic Regression - Weights: [[ 4.77107348]\n",
      " [-3.65488174]]\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "tx = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # Input features\n",
    "y_binary = np.array([0, 0, 0, 1, 1])  # Reshape to (5, 1)\n",
    "initial_w_log = np.zeros((tx.shape[1], 1))  # Initial weights for logistic regression (shape (D,1))\n",
    "lambda_ = 0.05\n",
    "max_iters = 10000  # Maximum iterations\n",
    "gamma = 0.01  # Learning rate\n",
    "lambda_ = 0.001  # Regularization parameter\n",
    "\n",
    "# Test Logistic Regression\n",
    "w_log, loss_log = reg_logistic_regression(y_binary, tx, lambda_, initial_w_log, max_iters, gamma)\n",
    "print(\"Logistic Regression - Loss:\", loss_log)\n",
    "print(\"Logistic Regression - Weights:\", w_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
