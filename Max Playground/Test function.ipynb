{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb582c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory that contains implementations.py\n",
    "sys.path.append(os.path.abspath(r\"../\"))\n",
    "\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e00964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ab6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "x, mean_x, std_x = standardize(X)\n",
    "tx = np.c_[np.ones((y.shape[0], 1)), x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232a5d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'std_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m std_x[std_x \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'std_x' is not defined"
     ]
    }
   ],
   "source": [
    "std_x[std_x == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795f4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial parameters\n",
    "max_iters = 1000  # Number of iterations\n",
    "gamma = 0.1  # Learning rate\n",
    "lambda_ = 0.0001  # Regularization parameter (lambda)\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c4ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba5e914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23637983897519196"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45185772",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Regularization parameter (lambda)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#print(initial_w)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model using regularized logistic regression\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m final_w, final_loss \u001b[38;5;241m=\u001b[39m reg_logistic_regression(y, X, lambda_, initial_w, max_iters, gamma)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#final_w, final_loss = log_learning_by_penalized_gradient(y, X, lambda_, initial_w, max_iters, gamma)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Output the final weights and the final loss\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#print(\"Final weights:\\n\", final_w)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#print(\"Final loss:\", final_loss)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(initial_w)\n",
      "File \u001b[1;32m~\\ml-project-1-overfitters-anonymous\\implementations.py:421\u001b[0m, in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# start the logistic regression\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# get loss and update w.\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m     w, loss \u001b[38;5;241m=\u001b[39m log_learning_by_penalized_gradient(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), tx, w, gamma, lambda_)\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# converge criterion\u001b[39;00m\n\u001b[0;32m    423\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32m~\\ml-project-1-overfitters-anonymous\\implementations.py:256\u001b[0m, in \u001b[0;36mlog_learning_by_penalized_gradient\u001b[1;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_learning_by_penalized_gradient\u001b[39m(y, tx, w, gamma, lambda_):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Do one step of gradient descent, using the penalized logistic regression.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    Return the loss and updated w.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m           [0.24228716]])\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     loss, gradient \u001b[38;5;241m=\u001b[39m penalized_logistic_regression(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), tx, w, lambda_)\n\u001b[0;32m    257\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m gradient\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, w\n",
      "File \u001b[1;32m~\\ml-project-1-overfitters-anonymous\\implementations.py:211\u001b[0m, in \u001b[0;36mpenalized_logistic_regression\u001b[1;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"return the loss and gradient.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m       [ 0.57712843]])\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# New vectorized loss computation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m sig_pred \u001b[38;5;241m=\u001b[39m sigmoid(tx \u001b[38;5;241m@\u001b[39m w)\n\u001b[0;32m    212\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m*\u001b[39m (y\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(sig_pred) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sig_pred))\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m (lambda_ \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (w[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m w[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Gradient with regularization (excluding bias term)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# Set initial parameters\n",
    "initial_w = np.random.randn(X.shape[1], 1) * 0.01  # Initialize w with zeros, shape (D, 1), where D is the number of features\n",
    "max_iters = 10  # Number of iterations\n",
    "gamma = 0.1  # Learning rate\n",
    "lambda_ = 0  # Regularization parameter (lambda)\n",
    "\n",
    "\n",
    "#print(initial_w)\n",
    "# Train the model using regularized logistic regression\n",
    "final_w, final_loss = reg_logistic_regression(y, X, lambda_, initial_w, max_iters, gamma)\n",
    "#final_w, final_loss = log_learning_by_penalized_gradient(y, X, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "# Output the final weights and the final loss\n",
    "#print(\"Final weights:\\n\", final_w)\n",
    "#print(\"Final loss:\", final_loss)\n",
    "print(initial_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = log_learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    return w, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
