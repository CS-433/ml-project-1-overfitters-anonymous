{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory that contains implementations.py\n",
    "sys.path.append(os.path.abspath(r\"../\"))\n",
    "\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from log_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths to X and y data\n",
    "X_path = '..\\\\data\\\\x_train.csv'\n",
    "y_path = '..\\\\data\\\\y_train.csv'\n",
    "\n",
    "# Load the data\n",
    "X, y = load_csv_data(X_path, y_path, frac=1)\n",
    "tx, y = clean_and_standardize(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22602841740052768\n",
      "Current iteration=200, loss=0.22519108811962496\n",
      "Current iteration=300, loss=0.2249908791699591\n",
      "Lambda=1e-07, Training loss=0.22490059273523597\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22669036554310892\n",
      "Current iteration=200, loss=0.22588363093835784\n",
      "Current iteration=300, loss=0.22568610905522601\n",
      "Lambda=1e-07, Training loss=0.22559495743375477\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2269676126484577\n",
      "Current iteration=200, loss=0.2261434701958467\n",
      "Current iteration=300, loss=0.22594004101390075\n",
      "Lambda=1e-07, Training loss=0.2258468571367743\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2262439982031484\n",
      "Current iteration=200, loss=0.22541787747409145\n",
      "Current iteration=300, loss=0.22521890306006714\n",
      "Lambda=1e-07, Training loss=0.22512852372292055\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22575414266402094\n",
      "Current iteration=200, loss=0.22492126295247442\n",
      "Current iteration=300, loss=0.2247198309914042\n",
      "Lambda=1e-07, Training loss=0.22462750568354456\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2272334284427585\n",
      "Current iteration=200, loss=0.2264276552822895\n",
      "Current iteration=300, loss=0.22623546595293462\n",
      "Lambda=1e-07, Training loss=0.22614878504878733\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22663626719425395\n",
      "Current iteration=200, loss=0.22580859135243994\n",
      "Current iteration=300, loss=0.22561044437516817\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.22552168518640223\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22643703468507023\n",
      "Current iteration=200, loss=0.22562569851729808\n",
      "Current iteration=300, loss=0.22543316195015425\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.22534611114027545\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2264017890989656\n",
      "Current iteration=200, loss=0.22556806476397342\n",
      "Current iteration=300, loss=0.2253637073240577\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.2252698695199096\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22688122670382985\n",
      "Current iteration=200, loss=0.22606841183131998\n",
      "Current iteration=300, loss=0.22587111579129004\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.22578011074122056\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2258160006912167\n",
      "Current iteration=200, loss=0.2249852827474894\n",
      "Current iteration=300, loss=0.22478134910598063\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.2246880895009605\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22671236142688703\n",
      "Current iteration=200, loss=0.22589530548677256\n",
      "Current iteration=300, loss=0.22569810662990888\n",
      "Lambda=1.6065060038537286e-07, Training loss=0.22560807021841128\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22594889378271243\n",
      "Current iteration=200, loss=0.22512157417799156\n",
      "Current iteration=300, loss=0.22492219066068722\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.2248325605749696\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22628841250303636\n",
      "Current iteration=200, loss=0.22547023907064012\n",
      "Current iteration=300, loss=0.2252736375435105\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.22518437356350335\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22573493269354727\n",
      "Current iteration=200, loss=0.22492044268026334\n",
      "Current iteration=300, loss=0.22472645461231577\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.22463765351457124\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22685725876555787\n",
      "Current iteration=200, loss=0.2260317603569195\n",
      "Current iteration=300, loss=0.2258272238762859\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.22573284524784853\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22694147147711502\n",
      "Current iteration=200, loss=0.22611805621921985\n",
      "Current iteration=300, loss=0.22591813357483526\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.22582741812242912\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22712567843410117\n",
      "Current iteration=200, loss=0.2263016785838557\n",
      "Current iteration=300, loss=0.22610177172398216\n",
      "Lambda=2.5808615404180767e-07, Training loss=0.22600944097261647\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22597472229030005\n",
      "Current iteration=200, loss=0.22514791865203612\n",
      "Current iteration=300, loss=0.2249508249789308\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.22486146658006673\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22636809026348714\n",
      "Current iteration=200, loss=0.22555909652333217\n",
      "Current iteration=300, loss=0.22536822318158953\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.2252817705035324\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22684789860639845\n",
      "Current iteration=200, loss=0.22604354365219254\n",
      "Current iteration=300, loss=0.22584652253944773\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.22575587976534928\n",
      "Current iteration=0, loss=0.693147180560063\n",
      "Current iteration=100, loss=0.22722338657113136\n",
      "Current iteration=200, loss=0.2263918822180707\n",
      "Current iteration=300, loss=0.2261880777926433\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.2260949000031873\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22709754897789214\n",
      "Current iteration=200, loss=0.22628462056969745\n",
      "Current iteration=300, loss=0.22608628760984137\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.2259951549773714\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22537702380229965\n",
      "Current iteration=200, loss=0.22452973893360556\n",
      "Current iteration=300, loss=0.2243239215170964\n",
      "Lambda=4.1461695597968144e-07, Training loss=0.22423085301250026\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22658037431533823\n",
      "Current iteration=200, loss=0.22576448786906939\n",
      "Current iteration=300, loss=0.22557026665306237\n",
      "Lambda=6.660846290809154e-07, Training loss=0.22548284681146052\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2267602004348254\n",
      "Current iteration=200, loss=0.22592708162320863\n",
      "Current iteration=300, loss=0.22572423977486972\n",
      "Lambda=6.660846290809154e-07, Training loss=0.2256312667541838\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2253215825761064\n",
      "Current iteration=200, loss=0.22450196608980627\n",
      "Current iteration=300, loss=0.2243031730271673\n",
      "Lambda=6.660846290809154e-07, Training loss=0.22421134089565767\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22616362464347695\n",
      "Current iteration=200, loss=0.22532621141346035\n",
      "Current iteration=300, loss=0.2251194975409371\n",
      "Lambda=6.660846290809154e-07, Training loss=0.22502474961615201\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22730078091712347\n",
      "Current iteration=200, loss=0.2264993200863734\n",
      "Current iteration=300, loss=0.22630945177701417\n",
      "Lambda=6.660846290809154e-07, Training loss=0.2262238869443838\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22676009751259563\n",
      "Current iteration=200, loss=0.22592996736320273\n",
      "Current iteration=300, loss=0.22572608964162333\n",
      "Lambda=6.660846290809154e-07, Training loss=0.22563259170507713\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22599495597703648\n",
      "Current iteration=200, loss=0.22516222402184316\n",
      "Current iteration=300, loss=0.22496111417833756\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.22487035839852193\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22711882489752502\n",
      "Current iteration=200, loss=0.22629790437875966\n",
      "Current iteration=300, loss=0.2260993546905752\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.22600891108302223\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22616210025686528\n",
      "Current iteration=200, loss=0.2253317374507653\n",
      "Current iteration=300, loss=0.22513089762615426\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.2250393416689493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22595392937172423\n",
      "Current iteration=200, loss=0.22512816944740127\n",
      "Current iteration=300, loss=0.22492514994943394\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.22483141915543617\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22725945515961957\n",
      "Current iteration=200, loss=0.2264400564881109\n",
      "Current iteration=300, loss=0.22623970737746998\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.22614843511098873\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22641870374048226\n",
      "Current iteration=200, loss=0.2256089368621623\n",
      "Current iteration=300, loss=0.2254156293070069\n",
      "Lambda=1.0700689556931746e-06, Training loss=0.22532703374035803\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22651708188991904\n",
      "Current iteration=200, loss=0.2256998023632183\n",
      "Current iteration=300, loss=0.22550445945699366\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22541619882866787\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22719065841662114\n",
      "Current iteration=200, loss=0.22636478345140007\n",
      "Current iteration=300, loss=0.22616245423042633\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22606927183196038\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22595784830719204\n",
      "Current iteration=200, loss=0.2251093986354032\n",
      "Current iteration=300, loss=0.2249014166363041\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22480650673377514\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22647697130096367\n",
      "Current iteration=200, loss=0.22565603022623956\n",
      "Current iteration=300, loss=0.22545721903781593\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22536719491087998\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2263299626017867\n",
      "Current iteration=200, loss=0.22551755192595274\n",
      "Current iteration=300, loss=0.225321686674885\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22523162211632855\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22645987665783626\n",
      "Current iteration=200, loss=0.2256521811731914\n",
      "Current iteration=300, loss=0.22545916106063826\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22537168176913416\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22631370177196997\n",
      "Current iteration=200, loss=0.22547579230052692\n",
      "Current iteration=300, loss=0.22526904441028706\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22517391373514326\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22628144918162776\n",
      "Current iteration=200, loss=0.2254424809139106\n",
      "Current iteration=300, loss=0.22523867602178174\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22514683992404802\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22614864838610543\n",
      "Current iteration=200, loss=0.2253305225038608\n",
      "Current iteration=300, loss=0.22513825656039915\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22505090957995108\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22593166643867788\n",
      "Current iteration=200, loss=0.22511762516474837\n",
      "Current iteration=300, loss=0.2249221701446039\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22483330496148232\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2270993183260605\n",
      "Current iteration=200, loss=0.2262831976770958\n",
      "Current iteration=300, loss=0.22608559937130182\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22599586510002662\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22710916959068173\n",
      "Current iteration=200, loss=0.22629984213815874\n",
      "Current iteration=300, loss=0.22610360231792231\n",
      "Lambda=2.761699813343849e-06, Training loss=0.22601397735411444\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22676825545542867\n",
      "Current iteration=200, loss=0.2259577654950868\n",
      "Current iteration=300, loss=0.22576016609052757\n",
      "Lambda=4.436687330978607e-06, Training loss=0.22567013466197341\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22593766543658333\n",
      "Current iteration=200, loss=0.22511527284524765\n",
      "Current iteration=300, loss=0.22491466172806238\n",
      "Lambda=4.436687330978607e-06, Training loss=0.2248217411481814\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22694506946463022\n",
      "Current iteration=200, loss=0.22610919739339091\n",
      "Current iteration=300, loss=0.22590379335717695\n",
      "Lambda=4.436687330978607e-06, Training loss=0.22581007835380432\n",
      "Current iteration=0, loss=0.6931471805600634\n",
      "Current iteration=100, loss=0.22578064366050474\n",
      "Current iteration=200, loss=0.22494866894191107\n",
      "Current iteration=300, loss=0.2247501544858111\n",
      "Lambda=4.436687330978607e-06, Training loss=0.22466002257324283\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2267713257398438\n",
      "Current iteration=200, loss=0.22596880913424575\n",
      "Current iteration=300, loss=0.22577738154629037\n",
      "Lambda=4.436687330978607e-06, Training loss=0.22569077258819722\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.226711306423457\n",
      "Current iteration=200, loss=0.22588237507706754\n",
      "Current iteration=300, loss=0.22568318623182942\n",
      "Lambda=4.436687330978607e-06, Training loss=0.22559333279695565\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22631432274409155\n",
      "Current iteration=200, loss=0.2254895386171023\n",
      "Current iteration=300, loss=0.22528909226906788\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22519787817336748\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22633665668244898\n",
      "Current iteration=200, loss=0.22550522082238447\n",
      "Current iteration=300, loss=0.22530308898208667\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22521032734151739\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22684299955846599\n",
      "Current iteration=200, loss=0.22601789147069706\n",
      "Current iteration=300, loss=0.22581734916360627\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22572597907709419\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22650948725834813\n",
      "Current iteration=200, loss=0.22568100275343603\n",
      "Current iteration=300, loss=0.2254786077708395\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22538585735263236\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22612284318053846\n",
      "Current iteration=200, loss=0.22531179405349253\n",
      "Current iteration=300, loss=0.22512026481526198\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22503382057447313\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2267985440199878\n",
      "Current iteration=200, loss=0.225986311234926\n",
      "Current iteration=300, loss=0.22578956306396206\n",
      "Lambda=7.127564834438907e-06, Training loss=0.22570020572876615\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22615122208742436\n",
      "Current iteration=200, loss=0.22533308091928075\n",
      "Current iteration=300, loss=0.22513393334472584\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.2250423972377785\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22674035383196553\n",
      "Current iteration=200, loss=0.2259083742796204\n",
      "Current iteration=300, loss=0.2257109654909336\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.2256224186748664\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22643443096582602\n",
      "Current iteration=200, loss=0.22561360765746458\n",
      "Current iteration=300, loss=0.22541155413747466\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.22531844087949288\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22670812558223769\n",
      "Current iteration=200, loss=0.22589688486632914\n",
      "Current iteration=300, loss=0.2256989517469244\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.2256081064318034\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2261424517894802\n",
      "Current iteration=200, loss=0.2253141892835081\n",
      "Current iteration=300, loss=0.22511680218174682\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.22502769599393546\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22676971423368275\n",
      "Current iteration=200, loss=0.22595321047351954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=300, loss=0.22575632876776364\n",
      "Lambda=1.1450475699382812e-05, Training loss=0.2256677146384515\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22736022471244371\n",
      "Current iteration=200, loss=0.2265513944997697\n",
      "Current iteration=300, loss=0.22635293696922593\n",
      "Lambda=1.839525795803971e-05, Training loss=0.2262615353758791\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22593784248616317\n",
      "Current iteration=200, loss=0.22511963690476772\n",
      "Current iteration=300, loss=0.22492451101035016\n",
      "Lambda=1.839525795803971e-05, Training loss=0.22483709631577098\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22628439660556582\n",
      "Current iteration=200, loss=0.22546161475667234\n",
      "Current iteration=300, loss=0.22526684773560068\n",
      "Lambda=1.839525795803971e-05, Training loss=0.22517944689444394\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.226518608608737\n",
      "Current iteration=200, loss=0.22570250490816837\n",
      "Current iteration=300, loss=0.22550577511079362\n",
      "Lambda=1.839525795803971e-05, Training loss=0.22541632665180084\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22649607070059866\n",
      "Current iteration=200, loss=0.22566436381964522\n",
      "Current iteration=300, loss=0.22546164783561723\n",
      "Lambda=1.839525795803971e-05, Training loss=0.2253690295800725\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2263887859618401\n",
      "Current iteration=200, loss=0.22557274980986217\n",
      "Current iteration=300, loss=0.22537596323017128\n",
      "Lambda=1.839525795803971e-05, Training loss=0.22528613617985574\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22660224947218302\n",
      "Current iteration=200, loss=0.225788975620567\n",
      "Current iteration=300, loss=0.22559454376580745\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.2255072739594401\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22594932544700266\n",
      "Current iteration=200, loss=0.22512353106597024\n",
      "Current iteration=300, loss=0.22492517275703716\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.22483620792653358\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22745600148579553\n",
      "Current iteration=200, loss=0.2266359994437882\n",
      "Current iteration=300, loss=0.2264368492106236\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.22634641190209467\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22590651442722073\n",
      "Current iteration=200, loss=0.22508817403919507\n",
      "Current iteration=300, loss=0.2248899537937053\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.22479921485469753\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22623057795479976\n",
      "Current iteration=200, loss=0.22541750689318468\n",
      "Current iteration=300, loss=0.22522396625319355\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.22513612826653415\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22686571741661618\n",
      "Current iteration=200, loss=0.22605648332068462\n",
      "Current iteration=300, loss=0.22586451161607027\n",
      "Lambda=2.9552092352028878e-05, Training loss=0.2257775581047505\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22581791046082902\n",
      "Current iteration=200, loss=0.22499860260786636\n",
      "Current iteration=300, loss=0.22480170953115428\n",
      "Lambda=4.747561378997416e-05, Training loss=0.22471271509949484\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22628560644769333\n",
      "Current iteration=200, loss=0.22546702948927308\n",
      "Current iteration=300, loss=0.22526912640498095\n",
      "Lambda=4.747561378997416e-05, Training loss=0.22517897281005306\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22722569306029883\n",
      "Current iteration=200, loss=0.22642032308058668\n",
      "Current iteration=300, loss=0.22622749307296447\n",
      "Lambda=4.747561378997416e-05, Training loss=0.22614008370641162\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2270038040598328\n",
      "Current iteration=200, loss=0.22618294019523233\n",
      "Current iteration=300, loss=0.2259857251360263\n",
      "Lambda=4.747561378997416e-05, Training loss=0.2258964130746307\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2268411661117043\n",
      "Current iteration=200, loss=0.22603018511821701\n",
      "Current iteration=300, loss=0.22583841965663534\n",
      "Lambda=4.747561378997416e-05, Training loss=0.22575312841809603\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2258402302727099\n",
      "Current iteration=200, loss=0.225027778378216\n",
      "Current iteration=300, loss=0.22483505307355697\n",
      "Lambda=4.747561378997416e-05, Training loss=0.22474829126678725\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22648215894925197\n",
      "Current iteration=200, loss=0.22566014964577663\n",
      "Current iteration=300, loss=0.22546617639843572\n",
      "Lambda=7.626985859023451e-05, Training loss=0.22538124191447864\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2270334799039937\n",
      "Current iteration=200, loss=0.2262594161474351\n",
      "Current iteration=300, loss=0.2260791373014597\n",
      "Lambda=7.626985859023451e-05, Training loss=0.22599781611948078\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.226705610514753\n",
      "Current iteration=200, loss=0.22589856965783836\n",
      "Current iteration=300, loss=0.22570346773021413\n",
      "Lambda=7.626985859023451e-05, Training loss=0.2256147515688029\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22658417779501927\n",
      "Current iteration=200, loss=0.22576837364553715\n",
      "Current iteration=300, loss=0.22557119471455533\n",
      "Lambda=7.626985859023451e-05, Training loss=0.22548180777001647\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22599260447256025\n",
      "Current iteration=200, loss=0.22517671002569953\n",
      "Current iteration=300, loss=0.2249861345947447\n",
      "Lambda=7.626985859023451e-05, Training loss=0.2249019807070094\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22633702884682916\n",
      "Current iteration=200, loss=0.22551540305265408\n",
      "Current iteration=300, loss=0.22531878741457148\n",
      "Lambda=7.626985859023451e-05, Training loss=0.2252298742918386\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22692784250098205\n",
      "Current iteration=200, loss=0.22613917433786387\n",
      "Current iteration=300, loss=0.22595310040610225\n",
      "Lambda=0.0001225279857382864, Training loss=0.22587036035640728\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22605759751979346\n",
      "Current iteration=200, loss=0.2252643560656938\n",
      "Current iteration=300, loss=0.2250756374552622\n",
      "Lambda=0.0001225279857382864, Training loss=0.22499057696138022\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22712938185061768\n",
      "Current iteration=200, loss=0.22633011254045013\n",
      "Current iteration=300, loss=0.22614313563320454\n",
      "Lambda=0.0001225279857382864, Training loss=0.2260604667645155\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2261353688063538\n",
      "Current iteration=200, loss=0.22532749171942046\n",
      "Current iteration=300, loss=0.22513948808761933\n",
      "Lambda=0.0001225279857382864, Training loss=0.22505583932515508\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2264041963278698\n",
      "Current iteration=200, loss=0.22559788242256132\n",
      "Current iteration=300, loss=0.22541251066028944\n",
      "Lambda=0.0001225279857382864, Training loss=0.22533181332374513\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22656397972960277\n",
      "Current iteration=200, loss=0.22575123273434883\n",
      "Current iteration=300, loss=0.22555862823809544\n",
      "Lambda=0.0001225279857382864, Training loss=0.2254736758086376\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22630902651932375\n",
      "Current iteration=200, loss=0.22554047160095744\n",
      "Current iteration=300, loss=0.22536531050122605\n",
      "Lambda=0.00019684194472866114, Training loss=0.22528958065440852\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22768960164503785\n",
      "Current iteration=200, loss=0.22689869483718939\n",
      "Current iteration=300, loss=0.22671220078363621\n",
      "Lambda=0.00019684194472866114, Training loss=0.22662975402278843\n",
      "Current iteration=0, loss=0.6931471805600632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=0.22689155625675544\n",
      "Current iteration=200, loss=0.22610282348476893\n",
      "Current iteration=300, loss=0.22591672660858678\n",
      "Lambda=0.00019684194472866114, Training loss=0.22583445066632002\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2263261166730348\n",
      "Current iteration=200, loss=0.22552677368298957\n",
      "Current iteration=300, loss=0.22534462815281556\n",
      "Lambda=0.00019684194472866114, Training loss=0.2252653758627655\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2259697356955067\n",
      "Current iteration=200, loss=0.22517397698123606\n",
      "Current iteration=300, loss=0.22499322450840323\n",
      "Lambda=0.00019684194472866114, Training loss=0.22491570767432198\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22631759139063928\n",
      "Current iteration=200, loss=0.2255300837632978\n",
      "Current iteration=300, loss=0.22535105819226386\n",
      "Lambda=0.00019684194472866114, Training loss=0.22527265997012058\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22695272917772089\n",
      "Current iteration=200, loss=0.22618985986915377\n",
      "Current iteration=300, loss=0.2260149051023235\n",
      "Lambda=0.00031622776601683794, Training loss=0.22593910778047033\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22668912073788788\n",
      "Current iteration=200, loss=0.22592890611092026\n",
      "Current iteration=300, loss=0.22576230581163334\n",
      "Lambda=0.00031622776601683794, Training loss=0.2256921966233095\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22623510721780146\n",
      "Current iteration=200, loss=0.22546357135490513\n",
      "Current iteration=300, loss=0.22528855095943803\n",
      "Lambda=0.00031622776601683794, Training loss=0.22521349667084653\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2267965734329937\n",
      "Current iteration=200, loss=0.22602809963153103\n",
      "Current iteration=300, loss=0.22586056688009024\n",
      "Lambda=0.00031622776601683794, Training loss=0.22579069295847753\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22642350315201784\n",
      "Current iteration=200, loss=0.22564150977642267\n",
      "Current iteration=300, loss=0.22546637576334558\n",
      "Lambda=0.00031622776601683794, Training loss=0.22539223141605427\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22676703737632048\n",
      "Current iteration=200, loss=0.2259975794413134\n",
      "Current iteration=300, loss=0.22582694129974656\n",
      "Lambda=0.00031622776601683794, Training loss=0.22575521302012796\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2262226564245766\n",
      "Current iteration=200, loss=0.22548101187634148\n",
      "Current iteration=300, loss=0.22532455740443552\n",
      "Lambda=0.0005080218046913018, Training loss=0.2252617347743127\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22664323145063736\n",
      "Current iteration=200, loss=0.2259255331689838\n",
      "Current iteration=300, loss=0.22577303408708138\n",
      "Lambda=0.0005080218046913018, Training loss=0.22571038570091348\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22666173216794264\n",
      "Current iteration=200, loss=0.22592486136282355\n",
      "Current iteration=300, loss=0.22576788852673554\n",
      "Lambda=0.0005080218046913018, Training loss=0.22570423338975293\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2273614731569125\n",
      "Current iteration=200, loss=0.2266230259443005\n",
      "Current iteration=300, loss=0.2264683303045826\n",
      "Lambda=0.0005080218046913018, Training loss=0.2264067612101681\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22653808667994044\n",
      "Current iteration=200, loss=0.22578378182283065\n",
      "Current iteration=300, loss=0.22561948702078244\n",
      "Lambda=0.0005080218046913018, Training loss=0.22555122470099587\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2270238009295517\n",
      "Current iteration=200, loss=0.22628090079825422\n",
      "Current iteration=300, loss=0.22612371851267019\n",
      "Lambda=0.0005080218046913018, Training loss=0.22606002033503367\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22684502409619905\n",
      "Current iteration=200, loss=0.2261617165859542\n",
      "Current iteration=300, loss=0.22602922991588706\n",
      "Lambda=0.0008161400793251826, Training loss=0.22597929115297213\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22760816069071046\n",
      "Current iteration=200, loss=0.2269262920257672\n",
      "Current iteration=300, loss=0.22679273188723464\n",
      "Lambda=0.0008161400793251826, Training loss=0.22674251852228483\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22671081149870764\n",
      "Current iteration=200, loss=0.22601148690620684\n",
      "Current iteration=300, loss=0.22587382930569733\n",
      "Lambda=0.0008161400793251826, Training loss=0.22582162663990635\n",
      "Current iteration=0, loss=0.6931471805600634\n",
      "Current iteration=100, loss=0.2261578553573161\n",
      "Current iteration=200, loss=0.22545982798285918\n",
      "Current iteration=300, loss=0.22532411244459227\n",
      "Lambda=0.0008161400793251826, Training loss=0.22527268555967936\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2271754393662195\n",
      "Current iteration=200, loss=0.22647527277411153\n",
      "Current iteration=300, loss=0.22633826223739734\n",
      "Lambda=0.0008161400793251826, Training loss=0.22628693718567336\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22689561341382294\n",
      "Current iteration=200, loss=0.2262068281229922\n",
      "Current iteration=300, loss=0.22606798592144414\n",
      "Lambda=0.0008161400793251826, Training loss=0.2260143076191734\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2279974264060502\n",
      "Current iteration=200, loss=0.22738740197567944\n",
      "Current iteration=300, loss=0.22728123695316713\n",
      "Lambda=0.001311133937421563, Training loss=0.2272449162536149\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22790223722749053\n",
      "Current iteration=200, loss=0.22727936280837407\n",
      "Current iteration=300, loss=0.2271743674032203\n",
      "Lambda=0.001311133937421563, Training loss=0.22713927178258175\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2265611197584147\n",
      "Current iteration=200, loss=0.2259254866416335\n",
      "Current iteration=300, loss=0.22581345617520623\n",
      "Lambda=0.001311133937421563, Training loss=0.225775151664007\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22654063493376814\n",
      "Current iteration=200, loss=0.22591948720083865\n",
      "Current iteration=300, loss=0.225811989380968\n",
      "Lambda=0.001311133937421563, Training loss=0.22577590616538964\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22641916460771205\n",
      "Current iteration=200, loss=0.22578453954584313\n",
      "Current iteration=300, loss=0.22567416611067342\n",
      "Lambda=0.001311133937421563, Training loss=0.22563668935755815\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22745414310190648\n",
      "Current iteration=200, loss=0.22682181533726517\n",
      "Current iteration=300, loss=0.22671113861872558\n",
      "Lambda=0.001311133937421563, Training loss=0.22667368089990125\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2268764846056154\n",
      "Current iteration=200, loss=0.22632520844334147\n",
      "Current iteration=300, loss=0.22624512154147786\n",
      "Lambda=0.00210634454232412, Training loss=0.22622241737488805\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22742128057287894\n",
      "Current iteration=200, loss=0.22688991321088964\n",
      "Current iteration=300, loss=0.22681743631225879\n",
      "Lambda=0.00210634454232412, Training loss=0.22679759331928034\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2279446407425367\n",
      "Current iteration=200, loss=0.22741864306768275\n",
      "Current iteration=300, loss=0.2273450758149308\n",
      "Lambda=0.00210634454232412, Training loss=0.22732440343823435\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2280001937083817\n",
      "Current iteration=200, loss=0.22746554767024493\n",
      "Current iteration=300, loss=0.2273893247171974\n",
      "Lambda=0.00210634454232412, Training loss=0.22736766258125699\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22749750846863678\n",
      "Current iteration=200, loss=0.226968876385136\n",
      "Current iteration=300, loss=0.2268931714064399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=0.00210634454232412, Training loss=0.2268715010138905\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22740320384912116\n",
      "Current iteration=200, loss=0.2268663915460649\n",
      "Current iteration=300, loss=0.2267902839331237\n",
      "Lambda=0.00210634454232412, Training loss=0.22676905692300794\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22835990757130378\n",
      "Current iteration=200, loss=0.22793037038077152\n",
      "Current iteration=300, loss=0.22788634267606345\n",
      "Lambda=0.003383855153428233, Training loss=0.22787728185649186\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.228605318082342\n",
      "Current iteration=200, loss=0.22818736678824572\n",
      "Current iteration=300, loss=0.2281444946929801\n",
      "Lambda=0.003383855153428233, Training loss=0.22813551942586513\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.228232709150572\n",
      "Current iteration=200, loss=0.22781519822098545\n",
      "Current iteration=300, loss=0.22777290840350914\n",
      "Lambda=0.003383855153428233, Training loss=0.2277640968483983\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22819916330758497\n",
      "Current iteration=200, loss=0.22777614857086076\n",
      "Current iteration=300, loss=0.22773216096066884\n",
      "Lambda=0.003383855153428233, Training loss=0.22772292777890935\n",
      "Current iteration=0, loss=0.6931471805600634\n",
      "Current iteration=100, loss=0.22639088710323002\n",
      "Current iteration=200, loss=0.22596381416626746\n",
      "Current iteration=300, loss=0.22592056277974037\n",
      "Lambda=0.003383855153428233, Training loss=0.22591173193693806\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.22883258666344491\n",
      "Current iteration=200, loss=0.22842542973397428\n",
      "Current iteration=300, loss=0.22838376368450064\n",
      "Lambda=0.003383855153428233, Training loss=0.2283749749399562\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22811815248969175\n",
      "Current iteration=200, loss=0.22782240492862493\n",
      "Current iteration=300, loss=0.22780350096411053\n",
      "Lambda=0.005436183620153837, Training loss=0.2278010193867492\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22895192440969905\n",
      "Current iteration=200, loss=0.22865331174299997\n",
      "Current iteration=300, loss=0.22863481028332358\n",
      "Lambda=0.005436183620153837, Training loss=0.22863254287710866\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.22882760294936874\n",
      "Current iteration=200, loss=0.22853261909360725\n",
      "Current iteration=300, loss=0.22851499259765312\n",
      "Lambda=0.005436183620153837, Training loss=0.22851291947166033\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.229813639827045\n",
      "Current iteration=200, loss=0.22952181961030987\n",
      "Current iteration=300, loss=0.22950386362417433\n",
      "Lambda=0.005436183620153837, Training loss=0.22950166995719753\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2287035914059938\n",
      "Current iteration=200, loss=0.22840715565232858\n",
      "Current iteration=300, loss=0.22838896814904305\n",
      "Lambda=0.005436183620153837, Training loss=0.22838676337943334\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22937052765555685\n",
      "Current iteration=200, loss=0.22908093016814265\n",
      "Current iteration=300, loss=0.2290632083457353\n",
      "Lambda=0.005436183620153837, Training loss=0.22906108098249603\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2305439368607266\n",
      "Current iteration=200, loss=0.23036093696324336\n",
      "Lambda=0.008733261623828438, Training loss=0.23035560716652045\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.22965095356200965\n",
      "Current iteration=200, loss=0.22946849118646864\n",
      "Lambda=0.008733261623828438, Training loss=0.22946330777369647\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23007521543824055\n",
      "Current iteration=200, loss=0.22989412496826941\n",
      "Lambda=0.008733261623828438, Training loss=0.22988887138031017\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23000616416654587\n",
      "Current iteration=200, loss=0.22982576173759042\n",
      "Lambda=0.008733261623828438, Training loss=0.2298206474070503\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.23041007468121358\n",
      "Current iteration=200, loss=0.23023296230192528\n",
      "Lambda=0.008733261623828438, Training loss=0.2302279114111626\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23056051549638212\n",
      "Current iteration=200, loss=0.23038483564579584\n",
      "Lambda=0.008733261623828438, Training loss=0.2303797869042491\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2314684209810183\n",
      "Current iteration=200, loss=0.23137022749260025\n",
      "Lambda=0.01403003723190572, Training loss=0.23136934159318995\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2316700076131231\n",
      "Current iteration=200, loss=0.23157406449257895\n",
      "Lambda=0.01403003723190572, Training loss=0.23157322411697537\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2323596160700373\n",
      "Current iteration=200, loss=0.23226340592659667\n",
      "Lambda=0.01403003723190572, Training loss=0.23226254274502212\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.23149865934319463\n",
      "Current iteration=200, loss=0.23140125092919137\n",
      "Lambda=0.01403003723190572, Training loss=0.23140037885612677\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23234883964533434\n",
      "Current iteration=200, loss=0.23225408066473927\n",
      "Lambda=0.01403003723190572, Training loss=0.23225325776103006\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23222650896585925\n",
      "Current iteration=200, loss=0.23213160093285076\n",
      "Lambda=0.01403003723190572, Training loss=0.2321307724759276\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2345146897854533\n",
      "Current iteration=200, loss=0.23446875664567163\n",
      "Lambda=0.022539339047347888, Training loss=0.23446873699475934\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23367767209639617\n",
      "Current iteration=200, loss=0.2336297964395351\n",
      "Lambda=0.022539339047347888, Training loss=0.23362976563970747\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23457221288875998\n",
      "Current iteration=200, loss=0.23452472798611187\n",
      "Lambda=0.022539339047347888, Training loss=0.23452469754668448\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2345279501884648\n",
      "Current iteration=200, loss=0.23448050059628206\n",
      "Lambda=0.022539339047347888, Training loss=0.23448047024893157\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23452905987559147\n",
      "Current iteration=200, loss=0.23448302377727898\n",
      "Lambda=0.022539339047347888, Training loss=0.23448300413523146\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2334587418796738\n",
      "Current iteration=200, loss=0.23341103860039464\n",
      "Lambda=0.022539339047347888, Training loss=0.23341100802647793\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23759150249773603\n",
      "Lambda=0.03620958350245917, Training loss=0.23756992107659325\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2368641346200352\n",
      "Lambda=0.03620958350245917, Training loss=0.23684216080607076\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23750249712523816\n",
      "Lambda=0.03620958350245917, Training loss=0.23748120430119615\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.23745047240039052\n",
      "Lambda=0.03620958350245917, Training loss=0.23742973799674041\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.23650464294783108\n",
      "Lambda=0.03620958350245917, Training loss=0.2364830625068997\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2370494802955465\n",
      "Lambda=0.03620958350245917, Training loss=0.23702809796982122\n",
      "Current iteration=0, loss=0.693147180560063\n",
      "Current iteration=100, loss=0.24192546224801842\n",
      "Lambda=0.058170913293743576, Training loss=0.24191709956111696\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24098576029850471\n",
      "Lambda=0.058170913293743576, Training loss=0.24097727184658663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.23997801122571802\n",
      "Lambda=0.058170913293743576, Training loss=0.23996906073878913\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24086682947153024\n",
      "Lambda=0.058170913293743576, Training loss=0.24085814963974556\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24040791579281995\n",
      "Lambda=0.058170913293743576, Training loss=0.24039891797606855\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2411243874438385\n",
      "Lambda=0.058170913293743576, Training loss=0.2411156885151842\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24553420811049914\n",
      "Lambda=0.09345192145605374, Training loss=0.2455313675400968\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24522162449070944\n",
      "Lambda=0.09345192145605374, Training loss=0.24521872471385753\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.24542682081655867\n",
      "Lambda=0.09345192145605374, Training loss=0.2454239029046718\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2452621226899411\n",
      "Lambda=0.09345192145605374, Training loss=0.24525919608514324\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.24582435052450224\n",
      "Lambda=0.09345192145605374, Training loss=0.24582147705117358\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2455747457376399\n",
      "Lambda=0.09345192145605374, Training loss=0.2455718607789984\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2510636107732394\n",
      "Lambda=0.15013107289081712, Training loss=0.25106289698800094\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.25096237568882873\n",
      "Lambda=0.15013107289081712, Training loss=0.25096168133955565\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.2501523159142964\n",
      "Lambda=0.15013107289081712, Training loss=0.25015158256412845\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.25176561500202027\n",
      "Lambda=0.15013107289081712, Training loss=0.25176496797098513\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.25072145653854744\n",
      "Lambda=0.15013107289081712, Training loss=0.25072074319447646\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.25124570069606394\n",
      "Lambda=0.15013107289081712, Training loss=0.2512450366818897\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2569675780575623\n",
      "Lambda=0.24118646996409948, Training loss=0.2569675013674706\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.257506518671026\n",
      "Lambda=0.24118646996409948, Training loss=0.2575064562415583\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Current iteration=100, loss=0.2577242962379218\n",
      "Lambda=0.24118646996409948, Training loss=0.2577242338607995\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2575150877994313\n",
      "Lambda=0.24118646996409948, Training loss=0.2575150257702594\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.25664999551417933\n",
      "Lambda=0.24118646996409948, Training loss=0.2566499181776447\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.25767769491362946\n",
      "Lambda=0.24118646996409948, Training loss=0.25767763194715704\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Lambda=0.3874675120456128, Training loss=0.26370345187508804\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Lambda=0.3874675120456128, Training loss=0.26445455152327757\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Lambda=0.3874675120456128, Training loss=0.26473033272951074\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Lambda=0.3874675120456128, Training loss=0.26338117043230824\n",
      "Current iteration=0, loss=0.6931471805600631\n",
      "Lambda=0.3874675120456128, Training loss=0.2653407782792437\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Lambda=0.3874675120456128, Training loss=0.2640612503826266\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.27460004368851815\n",
      "Current iteration=200, loss=0.27459981132305045\n",
      "Current iteration=300, loss=0.27459981132252104\n",
      "Lambda=0.622468884399544, Training loss=0.27701395669875667\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2749400246353039\n",
      "Current iteration=200, loss=0.2749396502376771\n",
      "Current iteration=300, loss=0.2749396502360179\n",
      "Lambda=0.622468884399544, Training loss=0.2771235636387127\n",
      "Current iteration=0, loss=0.6931471805600633\n",
      "Current iteration=100, loss=0.27450285057910784\n",
      "Current iteration=200, loss=0.2745026257651202\n",
      "Current iteration=300, loss=0.2745026257646219\n",
      "Lambda=0.622468884399544, Training loss=0.2769491927087958\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2750217923609456\n",
      "Current iteration=200, loss=0.2750215965006631\n",
      "Current iteration=300, loss=0.2750215965003124\n",
      "Lambda=0.622468884399544, Training loss=0.2775091630114722\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2742691441451193\n",
      "Current iteration=200, loss=0.27426874681748725\n",
      "Current iteration=300, loss=0.27426874681553787\n",
      "Lambda=0.622468884399544, Training loss=0.2763992100683706\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.27508455219938377\n",
      "Current iteration=200, loss=0.27508428770400206\n",
      "Current iteration=300, loss=0.27508428770327575\n",
      "Lambda=0.622468884399544, Training loss=0.27744822118194434\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.28152502325187534\n",
      "Current iteration=200, loss=0.28130500634423583\n",
      "Current iteration=300, loss=0.2818562373150448\n",
      "Lambda=1.0, Training loss=0.28344117998019175\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.28067613906720795\n",
      "Current iteration=200, loss=0.28246341515306733\n",
      "Current iteration=300, loss=0.28619192154141004\n",
      "Lambda=1.0, Training loss=0.2942431777799217\n",
      "Current iteration=0, loss=0.6931471805600634\n",
      "Current iteration=100, loss=0.279973514750983\n",
      "Current iteration=200, loss=0.28156769091059264\n",
      "Current iteration=300, loss=0.28493009487340093\n",
      "Lambda=1.0, Training loss=0.2921181061497287\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.28079035077632003\n",
      "Current iteration=200, loss=0.2812286772833203\n",
      "Current iteration=300, loss=0.28267343726966154\n",
      "Lambda=1.0, Training loss=0.28583377697831425\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2808882355346584\n",
      "Current iteration=200, loss=0.28111472113689673\n",
      "Current iteration=300, loss=0.28224769572327424\n",
      "Lambda=1.0, Training loss=0.28484076460461133\n",
      "Current iteration=0, loss=0.6931471805600632\n",
      "Current iteration=100, loss=0.2805350615325921\n",
      "Current iteration=200, loss=0.281055408533136\n",
      "Current iteration=300, loss=0.2826195301557428\n",
      "Lambda=1.0, Training loss=0.28600086327634555\n"
     ]
    }
   ],
   "source": [
    "# Set initial parameters\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "max_iters = 400  # Number of iterations\n",
    "gamma = 1  # Learning rate\n",
    "\n",
    "# Cross validation\n",
    "lambdas, losses_tr, losses_te = cross_validation_reg_log(y, tx, 6, np.logspace(-7, 0, 35), max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdas, losses_tr, label='Training loss')\n",
    "plt.plot(lambdas, losses_te, label = 'Test loss')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.legend()\n",
    "plt.savefig('Hyperparametertuning finetuned10e-7--10e0.png', dpi=369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = '.\\\\hyperparam_tuning\\\\max_iters_400_gamma_1_lams_logspace-7_0_35_k_6' \n",
    "save_array_as_csv(lambdas, 'lambdas', newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best lambda    \n",
    "best_lambda = lambdas[np.argmin(losses_te)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with best lambda, extra long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = best_lambda\n",
    "# Set initial parameters\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "max_iters = 100000  # Number of iterations\n",
    "gamma = 0.5  # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805600689\n",
      "Current iteration=100, loss=0.22933462271261212\n",
      "Current iteration=200, loss=0.22658520498308887\n",
      "Current iteration=300, loss=0.2259742427307593\n",
      "Current iteration=400, loss=0.22573875701115767\n",
      "Current iteration=500, loss=0.22561708758832935\n",
      "Current iteration=600, loss=0.22554202533267337\n",
      "Current iteration=700, loss=0.22549061271886675\n",
      "Current iteration=800, loss=0.2254531312434999\n",
      "Current iteration=900, loss=0.22542469445214572\n",
      "Current iteration=1000, loss=0.2254025045877077\n",
      "Current iteration=1100, loss=0.2253848069252999\n",
      "Current iteration=1200, loss=0.22537043129966303\n",
      "Current iteration=1300, loss=0.2253585644806532\n",
      "Current iteration=1400, loss=0.22534862480757661\n",
      "Current iteration=1500, loss=0.22534018733586605\n",
      "Current iteration=1600, loss=0.225332936395091\n",
      "Current iteration=1700, loss=0.22532663419838844\n",
      "Current iteration=1800, loss=0.22532109941737805\n",
      "Current iteration=1900, loss=0.22531619221983223\n",
      "Current iteration=2000, loss=0.225311803635736\n",
      "Current iteration=2100, loss=0.22530784789275873\n",
      "Current iteration=2200, loss=0.22530425682657493\n",
      "Current iteration=2300, loss=0.2253009757621718\n",
      "Current iteration=2400, loss=0.22529796045054978\n",
      "Current iteration=2500, loss=0.225295174770429\n",
      "Current iteration=2600, loss=0.22529258898955096\n",
      "Current iteration=2700, loss=0.22529017843875057\n",
      "Current iteration=2800, loss=0.22528792249290894\n",
      "Current iteration=2900, loss=0.22528580378179106\n",
      "Current iteration=3000, loss=0.22528380757435298\n",
      "Current iteration=3100, loss=0.22528192129488722\n",
      "Current iteration=3200, loss=0.22528013414007597\n",
      "Current iteration=3300, loss=0.22527843677380446\n",
      "Current iteration=3400, loss=0.22527682108230812\n",
      "Current iteration=3500, loss=0.225275279976442\n",
      "Current iteration=3600, loss=0.22527380723099322\n",
      "Current iteration=3700, loss=0.22527239735329813\n",
      "Current iteration=3800, loss=0.22527104547518473\n",
      "Current iteration=3900, loss=0.22526974726359553\n",
      "Current iteration=4000, loss=0.2252684988462551\n",
      "Current iteration=4100, loss=0.2252672967495241\n",
      "Current iteration=4200, loss=0.22526613784617988\n",
      "Current iteration=4300, loss=0.22526501931132245\n",
      "Current iteration=4400, loss=0.22526393858496924\n",
      "Current iteration=4500, loss=0.22526289334017915\n",
      "Lambda=1.7190722018585747e-06, Training loss=0.22526200118620274\n"
     ]
    }
   ],
   "source": [
    "final_w, final_loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '.\\\\final_results' \n",
    "name = 'resuts'\n",
    "X_path = '..\\\\data\\\\x_test.csv'\n",
    "y, _ = safe_results(final_w, X_path, path, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
