{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory that contains implementations.py\n",
    "sys.path.append(os.path.abspath(r\"../\"))\n",
    "\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max Tost\\ml-project-1-overfitters-anonymous\\log_helpers.py:67: RuntimeWarning: Mean of empty slice\n",
      "  col_means = np.nanmean(xtest, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from log_helpers import *\n",
    "# Paths to X and y data\n",
    "X_path = '..\\\\data\\\\x_train.csv'\n",
    "y_path = '..\\\\data\\\\y_train.csv'\n",
    "\n",
    "# Load the data\n",
    "X, y = load_csv_data(X_path, y_path, frac=0.01)\n",
    "tx, y = clean_and_standardize(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[-1.74964637e+00]\n",
      " [ 2.76557646e-03]\n",
      " [-4.33843228e-04]\n",
      " [ 6.77779976e-03]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set initial parameters\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    "max_iters = 100  # Number of iterations\n",
    "gamma = 0.1  # Learning rate\n",
    "lambda_ = 1  # Regularization parameter (lambda)\n",
    "\n",
    "\n",
    "print(initial_w[:4])\n",
    "# Train the model using regularized logistic regression\n",
    "final_w, final_loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "print(initial_w[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    Split the dataset based on the split ratio. If ratio is 0.8,\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number\n",
    "    of samples is not an integer, use np.floor to determine the\n",
    "    number of training samples. Also check the documentation for\n",
    "    np.random.permutation, it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the training data.\n",
    "        x_te: numpy array containing the testing data.\n",
    "        y_tr: numpy array containing the training labels.\n",
    "        y_te: numpy array containing the testing labels.\n",
    "\n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]),\n",
    "     array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    N = len(y)\n",
    "    \n",
    "    # Generate a random permutation of indices\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    \n",
    "    # Calculate the number of training samples\n",
    "    split_idx = int(np.floor(ratio * N))\n",
    "    \n",
    "    # Split the indices\n",
    "    train_indices = shuffled_indices[:split_idx]\n",
    "    test_indices = shuffled_indices[split_idx:]\n",
    "    \n",
    "    # Split the data according to the indices\n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "    \n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set column-wise.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)  # Compute the mean for each feature (column)\n",
    "    std_x = np.std(x, axis=0)  # Compute the standard deviation for each feature (column)\n",
    "    \n",
    "    # Avoid division by zero by replacing zero std values with 1 (or a very small number)\n",
    "    std_x[std_x == 0] = 1\n",
    "    \n",
    "    # Standardize each feature\n",
    "    x_standardized = (x - mean_x) / std_x\n",
    "    \n",
    "    return x_standardized, mean_x, std_x\n",
    "\n",
    "\n",
    "def clean_and_standardize (X, y):\n",
    "    \"\"\"\n",
    "    Making a numpy array from the pandas data frames and removing nans in X\n",
    "    \n",
    "    Args:\n",
    "    ...\n",
    "    \n",
    "    Return:\n",
    "    ..\n",
    "    \n",
    "    Example:\n",
    "    ..\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardisind the pandas data frame X\n",
    "    X, mean_x, std_x = standardize(X)\n",
    "\n",
    "    xtest = np.copy(X)\n",
    "\n",
    "    # Replace nan values in the columns by the column average and set colums to 0 when there is only nan\n",
    "    \n",
    "    # Step 1: Identify where NaN values are\n",
    "    nan_mask = np.isnan(xtest)\n",
    "\n",
    "    # Step 2: Compute the column means, ignoring NaN values\n",
    "    # This will return NaN for columns that are fully NaN\n",
    "    col_means = np.nanmean(xtest, axis=0)\n",
    "\n",
    "    # Step 3: Handle columns that are entirely NaN\n",
    "    # If the entire column is NaN, np.nanmean returns NaN; we replace those NaN means with a default value, e.g., 0\n",
    "    col_means = np.where(np.isnan(col_means), 0, col_means)  # Replace NaN means with 0 or any other default value\n",
    "\n",
    "    # Step 4: Replace NaN values with the column means\n",
    "    xtest[nan_mask] = np.take(col_means, np.where(nan_mask)[1])\n",
    "\n",
    "    # Add a column of ones to include the bias\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), xtest]\n",
    "    \n",
    "    # Convert y to a numpy array and put it in the right shape\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    # Reshape y to ensure it's a column vector\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return tx, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv_data(X_path, y_path, frac=1):\n",
    "    \"\"\"\n",
    "    Loads X (features) and y (labels) from two CSV files, \n",
    "    converts -1, 1 targets to 0, 1 targets, and returns only a fraction of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        X_path (str): Path to the CSV file containing the X data (features).\n",
    "        y_path (str): Path to the CSV file containing the y data (labels).\n",
    "        frac (float): Fraction of the data to return, between 0 and 1. Default is 1.0 (100% of the data).\n",
    "    \n",
    "    Returns:\n",
    "        X_df (pd.DataFrame): DataFrame of the feature data (X) with columns as features.\n",
    "        y_df (pd.DataFrame): DataFrame of the target labels (y) with columns as labels (converted to 0 and 1).\n",
    "    \"\"\"\n",
    "    # Load the X (features) data from the CSV file\n",
    "    X_df = pd.read_csv(X_path)\n",
    "    \n",
    "    # Load the y (target) data from the other CSV file\n",
    "    y_df = pd.read_csv(y_path)\n",
    "    \n",
    "    # Optional: Merge both DataFrames on 'Id' column to ensure correct alignment\n",
    "    data = pd.merge(X_df, y_df, on='Id')\n",
    "    \n",
    "    # Select only a fraction of the data if needed\n",
    "    if 0 < frac < 1:\n",
    "        data = data.sample(frac=frac, random_state=42)  # Use random_state for reproducibility\n",
    "    \n",
    "    # Separate X (features) and y (labels) after merging\n",
    "    y_df = data[['_MICHD']]\n",
    "    X_df = data.drop(columns=['_MICHD'])\n",
    "    \n",
    "    # Convert -1, 1 targets to 0, 1 targets using .loc to avoid SettingWithCopyWarning\n",
    "    y_df.loc[:, '_MICHD'] = y_df['_MICHD'].replace({-1: 0, 1: 1})\n",
    "    \n",
    "    return X_df, y_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
