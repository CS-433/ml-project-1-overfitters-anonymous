{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory that contains implementations.py\n",
    "sys.path.append(os.path.abspath(r\"../\"))\n",
    "\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv_data(X_path, y_path, frac=1.0):\n",
    "    \"\"\"\n",
    "    Loads X (features) and y (labels) from two CSV files, \n",
    "    converts -1, 1 targets to 0, 1 targets, and returns only a fraction of the data.\n",
    "    \n",
    "    Parameters:\n",
    "        X_path (str): Path to the CSV file containing the X data (features).\n",
    "        y_path (str): Path to the CSV file containing the y data (labels).\n",
    "        frac (float): Fraction of the data to return, between 0 and 1. Default is 1.0 (100% of the data).\n",
    "    \n",
    "    Returns:\n",
    "        X_df (pd.DataFrame): DataFrame of the feature data (X) with columns as features.\n",
    "        y_df (pd.DataFrame): DataFrame of the target labels (y) with columns as labels (converted to 0 and 1).\n",
    "    \"\"\"\n",
    "    # Load the X (features) data from the CSV file\n",
    "    X_df = pd.read_csv(X_path)\n",
    "    \n",
    "    # Load the y (target) data from the other CSV file\n",
    "    y_df = pd.read_csv(y_path)\n",
    "    \n",
    "    # Optional: Merge both DataFrames on 'Id' column to ensure correct alignment\n",
    "    data = pd.merge(X_df, y_df, on='Id')\n",
    "    \n",
    "    # Separate X (features) and y (labels) after merging\n",
    "    y_df = data[['_MICHD']]\n",
    "    X_df = data.drop(columns=['_MICHD'])\n",
    "    \n",
    "    # Convert -1, 1 targets to 0, 1 targets\n",
    "    y_df['_MICHD'] = y_df['_MICHD'].replace({-1: 0, 1: 1})\n",
    "    \n",
    "    # Select only a fraction of the data\n",
    "    if 0 < frac < 1:\n",
    "        data = data.sample(frac=frac, random_state=42)  # Use random_state for reproducibility\n",
    "        y_df = data[['_MICHD']]\n",
    "        X_df = data.drop(columns=['_MICHD'])\n",
    "    \n",
    "    return X_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to X and y data\n",
    "X_path = '..\\\\data\\\\x_train.csv'\n",
    "y_path = '..\\\\data\\\\y_train.csv'\n",
    "\n",
    "# Load the data\n",
    "X, y = load_csv_data(X_path, y_path, frac=0.01)\n",
    "\n",
    "# Impute NaN values with the mean\n",
    "X = X.fillna(X.mean())  # Impute missing values with the column mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((X.shape[0], 1)), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set column-wise.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)  # Compute the mean for each feature (column)\n",
    "    std_x = np.std(x, axis=0)  # Compute the standard deviation for each feature (column)\n",
    "    \n",
    "    # Avoid division by zero by replacing zero std values with 1 (or a very small number)\n",
    "    std_x[std_x == 0] = 1\n",
    "    \n",
    "    # Standardize each feature\n",
    "    x_standardized = (x - mean_x) / std_x\n",
    "    \n",
    "    return x_standardized, mean_x, std_x\n",
    "\n",
    "X, mean_x, std_x = standardize(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_df (features) and y_df (labels) are loaded and y_df has been converted to 0, 1 if needed\n",
    "#X = X.to_numpy()  # Convert to NumPy arrays if they are in DataFrame format\n",
    "#y = y.to_numpy()\n",
    "\n",
    "# Reshape y to ensure it's a column vector\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Set initial parameters\n",
    "initial_w = np.random.randn(X.shape[1], 1) * 0.01  # Initialize w with zeros, shape (D, 1), where D is the number of features\n",
    "max_iters = 1  # Number of iterations\n",
    "gamma = 0.1  # Learning rate\n",
    "lambda_ = 0  # Regularization parameter (lambda)\n",
    "\n",
    "\n",
    "print(initial_w)\n",
    "# Train the model using regularized logistic regression\n",
    "#final_w, final_loss = reg_logistic_regression(y, X, lambda_, initial_w, max_iters, gamma)\n",
    "final_w, final_loss = log_learning_by_penalized_gradient(y, X, lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "# Output the final weights and the final loss\n",
    "#print(\"Final weights:\\n\", final_w)\n",
    "#print(\"Final loss:\", final_loss)\n",
    "print(initial_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    Split the dataset based on the split ratio. If ratio is 0.8,\n",
    "    you will have 80% of your data set dedicated to training\n",
    "    and the rest dedicated to testing. If ratio times the number\n",
    "    of samples is not an integer, use np.floor to determine the\n",
    "    number of training samples. Also check the documentation for\n",
    "    np.random.permutation, it could be useful.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        y: numpy array of shape (N,).\n",
    "        ratio: scalar in [0,1]\n",
    "        seed: integer.\n",
    "\n",
    "    Returns:\n",
    "        x_tr: numpy array containing the training data.\n",
    "        x_te: numpy array containing the testing data.\n",
    "        y_tr: numpy array containing the training labels.\n",
    "        y_te: numpy array containing the testing labels.\n",
    "\n",
    "    >>> split_data(np.arange(13), np.arange(13), 0.8, 1)\n",
    "    (array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]),\n",
    "     array([ 2,  3,  4, 10,  1,  6,  0,  7, 12,  9]), array([ 8, 11,  5]))\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    N = len(y)\n",
    "    \n",
    "    # Generate a random permutation of indices\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    \n",
    "    # Calculate the number of training samples\n",
    "    split_idx = int(np.floor(ratio * N))\n",
    "    \n",
    "    # Split the indices\n",
    "    train_indices = shuffled_indices[:split_idx]\n",
    "    test_indices = shuffled_indices[split_idx:]\n",
    "    \n",
    "    # Split the data according to the indices\n",
    "    x_tr = x[train_indices]\n",
    "    y_tr = y[train_indices]\n",
    "    x_te = x[test_indices]\n",
    "    y_te = y[test_indices]\n",
    "    \n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
