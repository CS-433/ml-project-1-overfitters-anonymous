{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory that contains implementations.py\n",
    "sys.path.append(os.path.abspath(r\"../\"))\n",
    "\n",
    "from implementations import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "y = np.array([1, 2, 3, 4, 5])  # Output variable (target)\n",
    "tx = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # Input features\n",
    "initial_w = np.array([0.1, 0.1])  # Initial weights\n",
    "max_iters = 10000  # Maximum iterations\n",
    "gamma = 0.01  # Learning rate\n",
    "lambda_ = 0.001  # Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test Gradient Descent with Mean Squared Error\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m loss_gd, w_gd \u001b[38;5;241m=\u001b[39m mean_squared_error_gd(y, tx, initial_w, max_iters, gamma)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient Descent - Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_gd)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient Descent - Weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w_gd)\n",
      "File \u001b[1;32m~\\ml-project-1-overfitters-anonymous\\implementations.py:320\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[1;34m(y, tx, initial_w, max_iters, gamma, tol, verbose)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# make steps in the opposite direction of the gradient and stops if the parameters does not vary anymore ( i.e. we reached a minimum ), or if the maximum iteration number is reached. \u001b[39;00m\n\u001b[0;32m    319\u001b[0m step_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# the counter of steps\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step_count \u001b[38;5;241m<\u001b[39m max_iters \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(w \u001b[38;5;241m-\u001b[39m w_next) \u001b[38;5;241m>\u001b[39m tol : \u001b[38;5;66;03m# we could also instead check the condition abs(grad) > tol since we expect grad = 0 at minimum.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    322\u001b[0m     w \u001b[38;5;241m=\u001b[39m w_next \u001b[38;5;66;03m# w(t) is the w(t+1) of the step before. \u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Test Gradient Descent with Mean Squared Error\n",
    "loss_gd, w_gd = mean_squared_error_gd(y, tx, initial_w, max_iters, gamma)\n",
    "print(\"Gradient Descent - Loss:\", loss_gd)\n",
    "print(\"Gradient Descent - Weights:\", w_gd)\n",
    "\n",
    "# Test Stochastic Gradient Descent\n",
    "loss_sgd, w_sgd = mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma)\n",
    "print(\"Stochastic Gradient Descent - Loss:\", loss_sgd)\n",
    "print(\"Stochastic Gradient Descent - Weights:\", w_sgd)\n",
    "\n",
    "# Test Least Squares\n",
    "w_ls, loss_ls = least_squares(y, tx)\n",
    "print(\"Least Squares - Loss:\", loss_ls)\n",
    "print(\"Least Squares - Weights:\", w_ls)\n",
    "\n",
    "# Test Ridge Regression\n",
    "w_rr, loss_rr = ridge_regression(y, tx, lambda_)\n",
    "print(\"Ridge Regression - Loss:\", loss_rr)\n",
    "print(\"Ridge Regression - Weights:\", w_rr)\n",
    "\n",
    "# Test Logistic Regression\n",
    "y_binary = np.array([[0], [1], [0], [1], [0]])  # Example binary labels for logistic regression\n",
    "initial_w_log = np.zeros((tx.shape[1], 1))  # Initial weights for logistic regression\n",
    "w_log, loss_log = logistic_regression(y_binary, tx, initial_w_log, max_iters, gamma)\n",
    "print(\"Logistic Regression - Loss:\", loss_log)\n",
    "print(\"Logistic Regression - Weights:\", w_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.58257569495584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1, 2, -4])\n",
    "np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
